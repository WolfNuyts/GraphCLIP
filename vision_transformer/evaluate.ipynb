{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip.jt_ViT_RelClassifier import ViT_RelClassifier\n",
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes):\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, clip_model_type, clip_pretrained_dataset)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu()\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200)\n",
    "model.load_state_dict(torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-04-16/vision_transformer_0/checkpoints/best_rel_model.pt\"))\n",
    "# dataloader_train, dataloader_val = get_dataloader(prepocess_function,metadata_path,image_dir, testing_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationship predictions: tensor([ 2,  0,  0,  4,  1,  0,  4,  0,  0,  1,  1,  3,  0,  0, 13,  1,  1,  6,\n",
      "         0,  2,  4,  0,  4,  0,  0,  4,  0,  2,  2,  9,  4,  2, 16,  4,  0,  4,\n",
      "         0,  0,  0,  3,  2,  0,  0,  2,  0,  0,  0,  2,  0,  4,  3,  0,  0,  2,\n",
      "         4,  0,  0,  0,  0,  4,  0,  4,  0,  0], device='cuda:0')\n",
      "object 1 predictions: tensor([  4,  22,  13,   7,  64,  42,   0,  81,  83,  62, 163,  30,  37,  45,\n",
      "         13,   0,  54,   3,  28,   0,   7,  92,   0,  45,  11,   1,  47,  47,\n",
      "         85,  13,   0,   1,   9,  52,  11,   1,  21, 120,  48,  21,   1,  15,\n",
      "         51,  93,  72,  28, 104, 124,   2,  42,  30,  77, 128,  47,  42,   3,\n",
      "         19,  21,   5,   0,  76,  52,   2,   2], device='cuda:0')\n",
      "object 2 predictions: tensor([ 99,  37,   8,  65,  24, 124,   5,  17,  31,  79,   9,   0,  37,   6,\n",
      "         63,  18,  30,  27,   4, 124,  40,   6,   5,   4,  27,  40,  16,  16,\n",
      "         16, 104,   5, 163, 130,  44,  10,  29,   0,  12,  60,  73, 171,  13,\n",
      "          8, 135,  12,  55,  10, 171,  55,  90,  52,  12,  40,  16,  36,  34,\n",
      "         58,  42,   0,  36,  25,  40,   4,  55], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "### load the first batch and show the predictions\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "for (inputs, bounding_boxes, lrels, lobj1s, lobj2s) in dataloader_val:\n",
    "    rel, obj_1, obj_2 = model(inputs.to(device), bounding_boxes.to(device))\n",
    "    _, rel_preds = torch.max(rel, 1)\n",
    "    _, obj_1_preds = torch.max(obj_1, 1)\n",
    "    _, obj_2_preds = torch.max(obj_2, 1)\n",
    "    print(\"relationship predictions:\", rel_preds)\n",
    "    print(\"object 1 predictions:\", obj_1_preds)\n",
    "    print(\"object 2 predictions:\", obj_2_preds)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model doesn't always predict a constant which is very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_realistic_graphs_dataset_ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "accuracy: 0.8446601941747572\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir)\n",
    "correct = 0\n",
    "total = 0\n",
    "for d in list_to_iterate:\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    changed_edge = d['changed_edge']\n",
    "    image_id = original_graph.image_id\n",
    "    inputs, bounding_boxes, rel_label, obj1_label, obj2_label = dataset_orig.getitem_from_id_edge(image_id, changed_edge)\n",
    "    inputs2,_, rel_label_adv, obj1_label_adv, obj2_label_adv = dataset_adv.getitem_from_id_edge(image_id, changed_edge)\n",
    "    assert (inputs == inputs2).all()\n",
    "    # print(\"original relationship label:\", rel_label)\n",
    "    # print(\"adversarial relationship label:\", rel_label_adv)\n",
    "    rel, obj_1, obj_2 = model(inputs.unsqueeze(0).to(device), bounding_boxes.unsqueeze(0).to(device))\n",
    "    rel_adv, obj_1_adv, obj_2_adv = model(inputs.unsqueeze(0).to(device), bounding_boxes.unsqueeze(0).to(device))\n",
    "\n",
    "    original_rel_confidence = rel[0][rel_label].item()\n",
    "    adversarial_rel_confidence = rel_adv[0][rel_label_adv].item()\n",
    "    if original_rel_confidence > adversarial_rel_confidence:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
