{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "old_mode = False\n",
    "mode = \"text_embeddings\"\n",
    "\n",
    "def load_model_old(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes):\n",
    "    from open_clip.jt_ViT_RelClassifier import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, clip_model_type, clip_pretrained_dataset)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu()\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes, n_attr_classes, shallow=True, input_mode=\"text_embeddings\"):\n",
    "    from open_clip.jt_ViT_RelClassifier_lightning import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, n_attr_classes, clip_model_type, clip_pretrained_dataset, shallow=shallow, mode=input_mode)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu(min_mem=20000)\n",
    "    print(f\"Using device {device}\")\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/jthomm/anaconda3/envs/jtpython2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|█████████████████████████████████████| 1.71G/1.71G [00:35<00:00, 47.7MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text embeddings as input to the model.\n",
      "WARNING: occurence probabilities are not registered yet, but attributes are assumed to work better with a class weighted loss.\n",
      "Using device cuda:4\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'datamodule_hparams_name', 'datamodule_hyper_parameters'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-L-14' # 'ViT-L-14' #'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion400m_e32' # 'laion2b_s32b_b82k' #'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "\n",
    "if old_mode:\n",
    "    model, prepocess_function, device = load_model_old(clip_model_type, clip_pretrained_dataset, 100, 200)\n",
    "    model.load_state_dict(torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-04-16/vision_transformer_0/checkpoints/best_rel_model.pt\"))\n",
    "else:\n",
    "    model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200, 100)\n",
    "    # loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-05/vision_transformer_12/best_rel_model.ckpt\")\n",
    "    loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-09/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "    print(loaded.keys())\n",
    "    model.load_state_dict(loaded['state_dict'])\n",
    "# dataloader_train, dataloader_val = get_dataloader(prepocess_function,metadata_path,image_dir, testing_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the first batch and show the predictions\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "for (inputs, bounding_boxes, lrels, lobj1s, lobj2s) in dataloader_val:\n",
    "    rel, obj_1, obj_2 = model(inputs.to(device), bounding_boxes.to(device))\n",
    "    _, rel_preds = torch.max(rel, 1)\n",
    "    _, obj_1_preds = torch.max(obj_1, 1)\n",
    "    _, obj_2_preds = torch.max(obj_2, 1)\n",
    "    print(\"relationship predictions:\", rel_preds)\n",
    "    print(\"object 1 predictions:\", obj_1_preds)\n",
    "    print(\"object 2 predictions:\", obj_2_preds)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model doesn't always predict a constant which is very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered objects loaded from file\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "from jt_training import get_realistic_graphs_dataset_ViT\n",
    "dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode)\n",
    "print(len(list_to_iterate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered objects loaded from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/103 [00:00<00:05, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: sitting in adv rel: sitting on orig conf: -7.834031105041504 adv conf: -7.642735481262207\n",
      "orig rel: next to adv rel: on orig conf: -6.934695720672607 adv conf: -3.993264675140381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 20/103 [00:01<00:05, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: next to adv rel: on orig conf: -6.934689044952393 adv conf: -3.9932656288146973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/103 [00:01<00:04, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: in front of adv rel: on orig conf: -5.6098737716674805 adv conf: -2.5107221603393555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 54/103 [00:03<00:02, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: on adv rel: above orig conf: -4.416107654571533 adv conf: -1.0609261989593506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 82/103 [00:04<00:01, 19.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: on adv rel: behind orig conf: -5.231099605560303 adv conf: -3.944485902786255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 94/103 [00:05<00:00, 19.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: behind adv rel: next to orig conf: -3.344350814819336 adv conf: -3.322387218475342\n",
      "orig rel: above adv rel: behind orig conf: -2.1922268867492676 adv conf: 0.4060482382774353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 98/103 [00:05<00:00, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig rel: behind adv rel: next to orig conf: -3.3443384170532227 adv conf: -3.3224024772644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:05<00:00, 18.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.912621359223301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode)\n",
    "correct = 0\n",
    "total = 0\n",
    "for d in tqdm(list_to_iterate):\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    changed_edge = d['changed_edge']\n",
    "    image_id = original_graph.image_id\n",
    "    if old_mode:\n",
    "        inputs, bounding_boxes, rel_label, obj1_label, obj2_label = dataset_orig.getitem_from_id_edge(image_id, changed_edge)\n",
    "        inputs2,_, rel_label_adv, obj1_label_adv, obj2_label_adv = dataset_adv.getitem_from_id_edge(image_id, changed_edge)\n",
    "        assert (inputs == inputs2).all()\n",
    "    else:\n",
    "        image, full_text_clip_embd, rel_label, obj1_label, obj2_label, attr_label, rel_mask, attr_mask = dataset_orig.getitem_from_id_edge(image_id, changed_edge, mode=mode)\n",
    "        image2, _, rel_label_adv, obj1_label_adv, obj2_label_adv, attr_label_adv, rel_mask_adv, attr_mask_adv = dataset_adv.getitem_from_id_edge(image_id, changed_edge, mode=mode)\n",
    "        assert (image == image2).all()\n",
    "    # print(\"original relationship label:\", rel_label)\n",
    "    # print(\"adversarial relationship label:\", rel_label_adv)\n",
    "    if old_mode:\n",
    "        rel, obj_1, obj_2 = model(inputs.unsqueeze(0).to(device), bounding_boxes.unsqueeze(0).to(device))\n",
    "        rel_adv, obj_1_adv, obj_2_adv = model(inputs.unsqueeze(0).to(device), bounding_boxes.unsqueeze(0).to(device))\n",
    "    else:\n",
    "        # print(full_text_clip_embd.shape)\n",
    "        rel, obj1, obj2, attr = model(image.unsqueeze(0).to(device), full_text_clip_embd.unsqueeze(0).to(device))\n",
    "        rel_adv, obj1_adv, obj2_adv, attr_adv = model(image2.unsqueeze(0).to(device), full_text_clip_embd.unsqueeze(0).to(device))\n",
    "    original_rel_confidence = rel[0][rel_label].item()\n",
    "    adversarial_rel_confidence = rel_adv[0][rel_label_adv].item()\n",
    "    if original_rel_confidence > adversarial_rel_confidence:\n",
    "        correct += 1\n",
    "    else:\n",
    "        rel_name_dict = dataset_orig.rel_classes # a dict with the relationship names as keys and the ids as values\n",
    "        rel_name = [key for key, value in rel_name_dict.items() if value == rel_label][0]\n",
    "        rel_name_adv = [key for key, value in rel_name_dict.items() if value == rel_label_adv][0]\n",
    "        print(\"orig rel:\", rel_name, \"adv rel:\", rel_name_adv, \"orig conf:\", original_rel_confidence, \"adv conf:\", adversarial_rel_confidence)\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how many samples don't have any attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered graphs...\n",
      "Done loading filtered graphs.\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n"
     ]
    }
   ],
   "source": [
    "dataloader_train, dataloader_val = get_dataloader(prepocess_function,metadata_path,image_dir, testing_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [06:18<00:00,  8.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "how_many_attributes = np.zeros(100)\n",
    "for batch in tqdm(dataloader_val):\n",
    "    inputs, bounding_boxes, lrels, lobj1s, lobj2s, lattr, rel_mask = batch\n",
    "    for sample in range(len(lrels)):\n",
    "        attr = lattr[sample]\n",
    "        how_many_attributes[int(torch.sum(attr).item())] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_attributes = how_many_attributes[:np.nonzero(how_many_attributes)[0][-1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.5836e+04 7.0312e+04 1.8396e+04 5.8270e+03 2.1750e+03 7.0300e+02\n",
      " 2.2100e+02 3.7000e+01 3.5000e+01 1.0000e+00]\n",
      "[4.95166449e-01 3.63288778e-01 9.50486455e-02 3.01070046e-02\n",
      " 1.12378128e-02 3.63226776e-03 1.14186512e-03 1.91171988e-04\n",
      " 1.80838367e-04 5.16681048e-06]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# cut off trailing 0\n",
    "how_many_attributes2 = how_many_attributes[:np.nonzero(how_many_attributes)[0][-1]+1]\n",
    "print(how_many_attributes)\n",
    "print(how_many_attributes/np.sum(how_many_attributes))\n",
    "# print a histogram of the number of attributes per image\n",
    "plt.bar(range(len(how_many_attributes)), how_many_attributes)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how well a histogram predictor performs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the filtered graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/home/jthomm/GraphCLIP\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "import torch\n",
    "d = abspath(dirname('../'))\n",
    "print(d)\n",
    "sys.path.append(d)\n",
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from datasets.VG_graphs import get_realistic_graphs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_graphs_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "filtered_graphs = filtered_graphs = torch.load(filtered_graphs_path + \"filtered_graphs.pt\")\n",
    "list_to_iterate = get_realistic_graphs_dataset()\n",
    "ids_in_adv = [l['original_graph'].image_id for l in list_to_iterate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97216/97216 [00:01<00:00, 61986.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "histogram = {}\n",
    "for graph in tqdm(filtered_graphs):\n",
    "    if graph.image_id in ids_in_adv:\n",
    "        continue\n",
    "    for edge in graph.edges:\n",
    "        key = graph.nodes[edge[0]]['name'] + \" \" + graph.nodes[edge[1]]['name']\n",
    "        if key not in histogram:\n",
    "            histogram[key] = [graph.edges[edge]['predicate']]\n",
    "        else:\n",
    "            histogram[key].append(graph.edges[edge]['predicate'])\n",
    "# only keep the most common predicate per edge\n",
    "for key in histogram:\n",
    "    histogram[key] = histogram[key].count # max(set(histogram[key]), key=histogram[key].count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8543689320388349\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for d in list_to_iterate:\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    changed_edge = d['changed_edge']\n",
    "    image_id = original_graph.image_id\n",
    "    edge = original_graph.nodes[changed_edge[0]]['name'] + \" \" + original_graph.nodes[changed_edge[1]]['name']\n",
    "    orig_pred = original_graph.edges[changed_edge]['predicate']\n",
    "    adv_pred = adv_graph.edges[changed_edge]['predicate']\n",
    "\n",
    "    orig_predicate_confidence = histogram[edge](orig_pred)\n",
    "    adv_predicate_confidence = histogram[edge](adv_pred)\n",
    "    if orig_predicate_confidence > adv_predicate_confidence:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
