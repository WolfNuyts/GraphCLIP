{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the json to networkx graphs format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# append parent directory to path\n",
    "sys.path.append(\"..\")\n",
    "from datasets.VG_graphs import get_realistic_graphs_dataset\n",
    "dataset = get_realistic_graphs_dataset('v1', 'attr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "mode = \"text_embeddings\"\n",
    "\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes, n_attr_classes, shallow=True, input_mode=\"text_embeddings\", obj_heads=False):\n",
    "    from open_clip.jt_ViT_RelClassifier_lightning import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, n_attr_classes, clip_model_type, clip_pretrained_dataset, shallow=shallow, mode=input_mode, with_object_heads=obj_heads)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu(min_mem=20000)\n",
    "    print(f\"Using device {device}\")\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/jthomm/anaconda3/envs/jtpython2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text embeddings as input to the model.\n",
      "Using device cuda:0\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters', 'datamodule_hparams_name', 'datamodule_hyper_parameters'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-L-14' # 'ViT-L-14' #'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion2b_s32b_b82k' # 'laion2b_s32b_b82k' #'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "\n",
    "model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200, 100, obj_heads=True)\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-05/vision_transformer_12/best_rel_model.ckpt\")\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-09/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "# new two ViT/B32 models:\n",
    "# no attribute wighting\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-22/vision_transformer_8/best_rel_model.ckpt\", map_location=device)\n",
    "# attribute wighting\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-22/vision_transformer_7/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# new models with different learning rates\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-23/vision_transformer_4/best_rel_model.ckpt\", map_location=device)\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-23/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-24/vision_transformer_0/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# Big ViT-L\n",
    "loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-05-25/vision_transformer_6/best_rel_model.ckpt', map_location=torch.device('cpu'))\n",
    "print(loaded.keys())\n",
    "model.load_state_dict(loaded['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_OBJECTS = ['man', 'person', 'window', 'tree', 'building', 'shirt', 'wall', 'woman', 'sign', 'sky', 'ground', 'grass', 'table', 'pole', 'head', 'light', 'water', 'car', 'hand', 'hair', 'people', 'leg', 'trees', 'clouds', 'ear', 'plate', 'leaves', 'fence', 'door', 'pants', 'eye', 'train', 'chair', 'floor', 'road', 'street', 'hat', 'snow', 'wheel', 'shadow', 'jacket', 'nose', 'boy', 'line', 'shoe', 'clock', 'sidewalk', 'boat', 'tail', 'cloud', 'handle', 'letter', 'girl', 'leaf', 'horse', 'bus', 'helmet', 'bird', 'giraffe', 'field', 'plane', 'flower', 'elephant', 'umbrella', 'dog', 'shorts', 'arm', 'zebra', 'face', 'windows', 'sheep', 'glass', 'bag', 'cow', 'bench', 'cat', 'food', 'bottle', 'rock', 'tile', 'kite', 'tire', 'post', 'number', 'stripe', 'surfboard', 'truck', 'logo', 'glasses', 'roof', 'skateboard', 'motorcycle', 'picture', 'flowers', 'bear', 'player', 'foot', 'bowl', 'mirror', 'background', 'pizza', 'bike', 'shoes', 'spot', 'tracks', 'pillow', 'shelf', 'cap', 'mouth', 'box', 'jeans', 'dirt', 'lights', 'legs', 'house', 'part', 'trunk', 'banana', 'top', 'plant', 'cup', 'counter', 'board', 'bed', 'wave', 'bush', 'ball', 'sink', 'button', 'lamp', 'beach', 'brick', 'flag', 'neck', 'sand', 'vase', 'writing', 'wing', 'paper', 'seat', 'lines', 'reflection', 'coat', 'child', 'toilet', 'laptop', 'airplane', 'letters', 'glove', 'vehicle', 'phone', 'book', 'branch', 'sunglasses', 'edge', 'cake', 'desk', 'rocks', 'frisbee', 'tie', 'tower', 'animal', 'hill', 'mountain', 'headlight', 'ceiling', 'cabinet', 'eyes', 'stripes', 'wheels', 'lady', 'ocean', 'racket', 'container', 'skier', 'keyboard', 'towel', 'frame', 'windshield', 'hands', 'back', 'track', 'bat', 'finger', 'pot', 'orange', 'fork', 'waves', 'design', 'feet', 'basket', 'fruit', 'broccoli', 'engine', 'guy', 'knife', 'couch', 'railing', 'collar', 'cars']\n",
    "FILTERED_RELATIONSHIPS = ['on', 'has', 'in', 'of', 'wearing', 'with', 'behind', 'holding', 'on a', 'near', 'on top of', 'next to', 'has a', 'under', 'of a', 'by', 'above', 'wears', 'in front of', 'sitting on', 'on side of', 'attached to', 'wearing a', 'in a', 'over', 'are on', 'at', 'for', 'around', 'beside', 'standing on', 'riding', 'standing in', 'inside', 'have', 'hanging on', 'walking on', 'on front of', 'are in', 'hanging from', 'carrying', 'holds', 'covering', 'belonging to', 'between', 'along', 'eating', 'and', 'sitting in', 'watching', 'below', 'painted on', 'laying on', 'against', 'playing', 'from', 'inside of', 'looking at', 'with a', 'parked on', 'to', 'has an', 'made of', 'covered in', 'mounted on', 'says', 'growing on', 'across', 'part of', 'on back of', 'flying in', 'outside', 'lying on', 'worn by', 'walking in', 'sitting at', 'printed on', 'underneath', 'crossing', 'beneath', 'full of', 'using', 'filled with', 'hanging in', 'covered with', 'built into', 'standing next to', 'adorning', 'a', 'in middle of', 'flying', 'supporting', 'touching', 'next', 'swinging', 'pulling', 'growing in', 'sitting on top of', 'standing', 'lying on top of']\n",
    "FILTERED_ATTRIBUTES = ['white', 'black', 'blue', 'green', 'red', 'brown', 'yellow', 'small', 'large', 'wooden', 'gray', 'silver', 'metal', 'orange', 'grey', 'tall', 'long', 'dark', 'pink', 'clear', 'standing', 'round', 'tan', 'glass', 'here', 'wood', 'open', 'purple', 'big', 'short', 'plastic', 'parked', 'sitting', 'walking', 'striped', 'brick', 'young', 'gold', 'old', 'hanging', 'empty', 'on', 'bright', 'concrete', 'cloudy', 'colorful', 'one', 'beige', 'bare', 'wet', 'light', 'square', 'little', 'closed', 'stone', 'blonde', 'shiny', 'thin', 'dirty', 'flying', 'smiling', 'painted', 'thick', 'part', 'sliced', 'playing', 'tennis', 'calm', 'leather', 'distant', 'rectangular', 'looking', 'grassy', 'dry', 'light brown', 'cement', 'leafy', 'wearing', 'tiled', \"man's\", 'light blue', 'baseball', 'cooked', 'pictured', 'curved', 'decorative', 'dead', 'eating', 'paper', 'paved', 'fluffy', 'lit', 'back', 'framed', 'plaid', 'dirt', 'watching', 'colored', 'stuffed', 'circular']\n",
    "rel_classes = {rel:i for i,rel in enumerate(FILTERED_RELATIONSHIPS)}\n",
    "obj_classes = {obj:i for i,obj in enumerate(FILTERED_OBJECTS)}\n",
    "attr_classes = {attr:i for i,attr in enumerate(FILTERED_ATTRIBUTES)}\n",
    "import os\n",
    "obj_embeddings = torch.load(os.path.join('/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed', 'filtered_object_label_embeddings.pt'), map_location=device)\n",
    "text_embeddings = {obj:torch.tensor(obj_embeddings[i]) for i,obj in enumerate(FILTERED_OBJECTS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "# dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode, version='v2')\n",
    "correct = 0\n",
    "total = 0\n",
    "for d in tqdm(dataset):\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    image_id = original_graph.image_id\n",
    "\n",
    "    # load image\n",
    "    image = PIL.Image.open(image_dir + str(image_id) + \".jpg\").convert(\"RGB\")\n",
    "    image = prepocess_function(image).unsqueeze(0).to(device)\n",
    "\n",
    "    g_attr_confidences = []\n",
    "    def get_attr_confidence(g):\n",
    "        for node in g.nodes:\n",
    "            text_embd_obj = text_embeddings[g.nodes[node]['name']].to(device)\n",
    "            full_text_clip_embd = torch.cat((text_embd_obj, text_embd_obj), dim=0).reshape(1,-1)\n",
    "            obj_label = torch.tensor(obj_classes[g.nodes[node]['name']])\n",
    "            rel, obj1, obj2, attr1, attr2 = model(image, full_text_clip_embd.unsqueeze(0))\n",
    "            attr = torch.sigmoid(attr1)\n",
    "            attr_labels = [attr_classes[attr] for attr in g.nodes[node]['attributes']]\n",
    "            for attr_label in attr_labels:\n",
    "                g_attr_confidences.append(attr[0][attr_label].item())\n",
    "        if len(g_attr_confidences) > 0:\n",
    "            attr_confidence = torch.mean(torch.tensor(g_attr_confidences)).cpu().item()\n",
    "        else:\n",
    "            assert False, \"no attributes found\"\n",
    "        return attr_confidence\n",
    "    \n",
    "    original_attr_confidence = get_attr_confidence(original_graph)\n",
    "    adv_attr_confidence = get_attr_confidence(adv_graph)\n",
    "\n",
    "    if original_attr_confidence > adv_attr_confidence:\n",
    "        correct += 1\n",
    "    else:\n",
    "        pass\n",
    "        # rel_name_dict = dataset_orig.rel_classes # a dict with the relationship names as keys and the ids as values\n",
    "        # rel_name = [key for key, value in rel_name_dict.items() if value == rel_label][0]\n",
    "        # rel_name_adv = [key for key, value in rel_name_dict.items() if value == rel_label_adv][0]\n",
    "        # print(\"orig rel:\", rel_name, \"adv rel:\", rel_name_adv, \"orig conf:\", original_rel_confidence, \"adv conf:\", adversarial_rel_confidence)\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
