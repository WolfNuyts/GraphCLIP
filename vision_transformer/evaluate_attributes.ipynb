{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the json to networkx graphs format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "PATH = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/\"\n",
    "# load json file\n",
    "with open(PATH + \"realistic_adversarial_attributes_gt_accepted.json\") as f:\n",
    "    gt = json.load(f)\n",
    "with open(PATH + \"realistic_adversarial_attributes_adv_accepted.json\") as f:\n",
    "    adv = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "filtered_graphs_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "filtered_graphs = filtered_graphs = torch.load(filtered_graphs_path + \"filtered_graphs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97216/97216 [00:00<00:00, 1072840.61it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "150323",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     obj1 \u001b[39m=\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mobjects\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m     obj2 \u001b[39m=\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mobjects\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m     dataset_graphs\u001b[39m.\u001b[39mappend((graph_dict[image_id], obj1, obj2,\u001b[39m\"\u001b[39m\u001b[39mgt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m adv:\n\u001b[1;32m     12\u001b[0m     image_id \u001b[39m=\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 150323"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "graph_dict = {}\n",
    "for g in tqdm(filtered_graphs):\n",
    "    graph_dict[g.image_id] = g\n",
    "dataset_graphs = []\n",
    "for sample in gt:\n",
    "    image_id = sample[\"image_id\"]\n",
    "    obj1 = sample[\"objects\"][0]\n",
    "    obj2 = sample[\"objects\"][1]\n",
    "    dataset_graphs.append((graph_dict[image_id], obj1, obj2,\"gt\"))\n",
    "for sample in adv:\n",
    "    image_id = sample[\"image_id\"]\n",
    "    obj1 = sample[\"objects\"][0]\n",
    "    obj2 = sample[\"objects\"][1]\n",
    "    dataset_graphs.append((graph_dict[image_id], obj1, obj2,\"adv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "mode = \"text_embeddings\"\n",
    "\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes, n_attr_classes, shallow=True, input_mode=\"text_embeddings\"):\n",
    "    from open_clip.jt_ViT_RelClassifier_lightning import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, n_attr_classes, clip_model_type, clip_pretrained_dataset, shallow=shallow, mode=input_mode)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu(min_mem=20000)\n",
    "    print(f\"Using device {device}\")\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-B/32' # 'ViT-L-14' #'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion400m_e32' # 'laion2b_s32b_b82k' #'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "\n",
    "model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200, 100)\n",
    "loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-05/vision_transformer_12/best_rel_model.ckpt\")\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-09/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "print(loaded.keys())\n",
    "model.load_state_dict(loaded['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_realistic_graphs_dataset_ViT\n",
    "dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode, version='v2')\n",
    "print(len(list_to_iterate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode, version='v2')\n",
    "correct = 0\n",
    "total = 0\n",
    "for d in tqdm(list_to_iterate):\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    changed_edge = d['changed_edge']\n",
    "    image_id = original_graph.image_id\n",
    "\n",
    "    image, full_text_clip_embd, rel_label, obj1_label, obj2_label, attr_label, rel_mask, attr_mask = dataset_orig.getitem_from_id_edge(image_id, changed_edge, mode=mode)\n",
    "    image2, _, rel_label_adv, obj1_label_adv, obj2_label_adv, attr_label_adv, rel_mask_adv, attr_mask_adv = dataset_adv.getitem_from_id_edge(image_id, changed_edge, mode=mode)\n",
    "    assert (image == image2).all()\n",
    "    # print(\"original relationship label:\", rel_label)\n",
    "    # print(\"adversarial relationship label:\", rel_label_adv)\n",
    "    # print(full_text_clip_embd.shape)\n",
    "    rel, obj1, obj2, attr = model(image.unsqueeze(0).to(device), full_text_clip_embd.unsqueeze(0).to(device))\n",
    "    rel_adv, obj1_adv, obj2_adv, attr_adv = model(image2.unsqueeze(0).to(device), full_text_clip_embd.unsqueeze(0).to(device))\n",
    "    original_rel_confidence = rel[0][rel_label].item()\n",
    "    adversarial_rel_confidence = rel_adv[0][rel_label_adv].item()\n",
    "    if original_rel_confidence > adversarial_rel_confidence:\n",
    "        correct += 1\n",
    "    else:\n",
    "        rel_name_dict = dataset_orig.rel_classes # a dict with the relationship names as keys and the ids as values\n",
    "        rel_name = [key for key, value in rel_name_dict.items() if value == rel_label][0]\n",
    "        rel_name_adv = [key for key, value in rel_name_dict.items() if value == rel_label_adv][0]\n",
    "        print(\"orig rel:\", rel_name, \"adv rel:\", rel_name_adv, \"orig conf:\", original_rel_confidence, \"adv conf:\", adversarial_rel_confidence)\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
