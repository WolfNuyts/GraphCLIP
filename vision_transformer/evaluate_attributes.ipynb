{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the json to networkx graphs format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# append parent directory to path\n",
    "sys.path.append(\"..\")\n",
    "from datasets.VG_graphs import get_realistic_graphs_dataset\n",
    "dataset = get_realistic_graphs_dataset('v1', 'attr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "mode = \"text_embeddings\"\n",
    "\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes, n_attr_classes, shallow=True, input_mode=\"text_embeddings\", obj_heads=False):\n",
    "    from open_clip.jt_ViT_RelClassifier_lightning import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, n_attr_classes, clip_model_type, clip_pretrained_dataset, shallow=shallow, mode=input_mode, with_object_heads=obj_heads)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu(min_mem=20000)\n",
    "    print(f\"Using device {device}\")\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text embeddings as input to the model.\n",
      "Using device cuda:7\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters', 'datamodule_hparams_name', 'datamodule_hyper_parameters'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-L-14' # 'ViT-L-14' #'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion2b_s32b_b82k' # 'laion2b_s32b_b82k' #'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "\n",
    "model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200, 100, obj_heads=True)\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-05/vision_transformer_12/best_rel_model.ckpt\")\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-09/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "# new two ViT/B32 models:\n",
    "# no attribute wighting\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-22/vision_transformer_8/best_rel_model.ckpt\", map_location=device)\n",
    "# attribute wighting\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-22/vision_transformer_7/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# new models with different learning rates\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-23/vision_transformer_4/best_rel_model.ckpt\", map_location=device)\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-23/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-24/vision_transformer_0/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# Big ViT-L\n",
    "# loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-05-25/vision_transformer_6/best_rel_model.ckpt', map_location=torch.device('cpu'))\n",
    "loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-05-27/vision_transformer_39/model_epoch-v8.ckpt', map_location=torch.device('cpu'))\n",
    "print(loaded.keys())\n",
    "model.load_state_dict(loaded['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_OBJECTS = ['man', 'person', 'window', 'tree', 'building', 'shirt', 'wall', 'woman', 'sign', 'sky', 'ground', 'grass', 'table', 'pole', 'head', 'light', 'water', 'car', 'hand', 'hair', 'people', 'leg', 'trees', 'clouds', 'ear', 'plate', 'leaves', 'fence', 'door', 'pants', 'eye', 'train', 'chair', 'floor', 'road', 'street', 'hat', 'snow', 'wheel', 'shadow', 'jacket', 'nose', 'boy', 'line', 'shoe', 'clock', 'sidewalk', 'boat', 'tail', 'cloud', 'handle', 'letter', 'girl', 'leaf', 'horse', 'bus', 'helmet', 'bird', 'giraffe', 'field', 'plane', 'flower', 'elephant', 'umbrella', 'dog', 'shorts', 'arm', 'zebra', 'face', 'windows', 'sheep', 'glass', 'bag', 'cow', 'bench', 'cat', 'food', 'bottle', 'rock', 'tile', 'kite', 'tire', 'post', 'number', 'stripe', 'surfboard', 'truck', 'logo', 'glasses', 'roof', 'skateboard', 'motorcycle', 'picture', 'flowers', 'bear', 'player', 'foot', 'bowl', 'mirror', 'background', 'pizza', 'bike', 'shoes', 'spot', 'tracks', 'pillow', 'shelf', 'cap', 'mouth', 'box', 'jeans', 'dirt', 'lights', 'legs', 'house', 'part', 'trunk', 'banana', 'top', 'plant', 'cup', 'counter', 'board', 'bed', 'wave', 'bush', 'ball', 'sink', 'button', 'lamp', 'beach', 'brick', 'flag', 'neck', 'sand', 'vase', 'writing', 'wing', 'paper', 'seat', 'lines', 'reflection', 'coat', 'child', 'toilet', 'laptop', 'airplane', 'letters', 'glove', 'vehicle', 'phone', 'book', 'branch', 'sunglasses', 'edge', 'cake', 'desk', 'rocks', 'frisbee', 'tie', 'tower', 'animal', 'hill', 'mountain', 'headlight', 'ceiling', 'cabinet', 'eyes', 'stripes', 'wheels', 'lady', 'ocean', 'racket', 'container', 'skier', 'keyboard', 'towel', 'frame', 'windshield', 'hands', 'back', 'track', 'bat', 'finger', 'pot', 'orange', 'fork', 'waves', 'design', 'feet', 'basket', 'fruit', 'broccoli', 'engine', 'guy', 'knife', 'couch', 'railing', 'collar', 'cars']\n",
    "FILTERED_RELATIONSHIPS = ['on', 'has', 'in', 'of', 'wearing', 'with', 'behind', 'holding', 'on a', 'near', 'on top of', 'next to', 'has a', 'under', 'of a', 'by', 'above', 'wears', 'in front of', 'sitting on', 'on side of', 'attached to', 'wearing a', 'in a', 'over', 'are on', 'at', 'for', 'around', 'beside', 'standing on', 'riding', 'standing in', 'inside', 'have', 'hanging on', 'walking on', 'on front of', 'are in', 'hanging from', 'carrying', 'holds', 'covering', 'belonging to', 'between', 'along', 'eating', 'and', 'sitting in', 'watching', 'below', 'painted on', 'laying on', 'against', 'playing', 'from', 'inside of', 'looking at', 'with a', 'parked on', 'to', 'has an', 'made of', 'covered in', 'mounted on', 'says', 'growing on', 'across', 'part of', 'on back of', 'flying in', 'outside', 'lying on', 'worn by', 'walking in', 'sitting at', 'printed on', 'underneath', 'crossing', 'beneath', 'full of', 'using', 'filled with', 'hanging in', 'covered with', 'built into', 'standing next to', 'adorning', 'a', 'in middle of', 'flying', 'supporting', 'touching', 'next', 'swinging', 'pulling', 'growing in', 'sitting on top of', 'standing', 'lying on top of']\n",
    "FILTERED_ATTRIBUTES = ['white', 'black', 'blue', 'green', 'red', 'brown', 'yellow', 'small', 'large', 'wooden', 'gray', 'silver', 'metal', 'orange', 'grey', 'tall', 'long', 'dark', 'pink', 'clear', 'standing', 'round', 'tan', 'glass', 'here', 'wood', 'open', 'purple', 'big', 'short', 'plastic', 'parked', 'sitting', 'walking', 'striped', 'brick', 'young', 'gold', 'old', 'hanging', 'empty', 'on', 'bright', 'concrete', 'cloudy', 'colorful', 'one', 'beige', 'bare', 'wet', 'light', 'square', 'little', 'closed', 'stone', 'blonde', 'shiny', 'thin', 'dirty', 'flying', 'smiling', 'painted', 'thick', 'part', 'sliced', 'playing', 'tennis', 'calm', 'leather', 'distant', 'rectangular', 'looking', 'grassy', 'dry', 'light brown', 'cement', 'leafy', 'wearing', 'tiled', \"man's\", 'light blue', 'baseball', 'cooked', 'pictured', 'curved', 'decorative', 'dead', 'eating', 'paper', 'paved', 'fluffy', 'lit', 'back', 'framed', 'plaid', 'dirt', 'watching', 'colored', 'stuffed', 'circular']\n",
    "rel_classes = {rel:i for i,rel in enumerate(FILTERED_RELATIONSHIPS)}\n",
    "obj_classes = {obj:i for i,obj in enumerate(FILTERED_OBJECTS)}\n",
    "attr_classes = {attr:i for i,attr in enumerate(FILTERED_ATTRIBUTES)}\n",
    "import os\n",
    "obj_embeddings = torch.load(os.path.join('/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed', 'filtered_object_label_embeddings.pt'), map_location=device)\n",
    "text_embeddings = {obj:torch.tensor(obj_embeddings[i]) for i,obj in enumerate(FILTERED_OBJECTS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9\n",
      "[0.5067164301872253, 0.5126970410346985, 0.014940398745238781, 0.053423136472702026, 0.35332536697387695, 0.532669186592102, 0.5962006449699402, 0.43216270208358765, 0.8438829779624939, 0.1987074315547943, 0.40836283564567566, 0.30747613310813904, 0.014071589335799217, 0.30224454402923584, 0.3189968764781952, 0.870049774646759, 0.4589889645576477, 0.48020416498184204, 0.4660971760749817, 0.39960652589797974, 0.8092999458312988, 0.1367560774087906, 0.9554212689399719, 0.3491665720939636, 0.18109504878520966, 0.473165363073349, 0.26860034465789795, 0.4498293399810791, 0.2727140188217163, 0.15753072500228882, 0.45829343795776367, 0.037026457488536835, 0.2376910150051117, 0.26526564359664917, 0.09565319865942001, 0.2229175865650177, 0.010786309838294983, 0.8500232696533203, 0.0007952185114845634, 0.8031555414199829, 0.6283318996429443, 0.4280907213687897, 0.5244888067245483, 0.8952549695968628, 0.6321170330047607, 0.3447175920009613, 0.25000450015068054, 0.3405395746231079, 0.21550188958644867, 0.18403968214988708, 0.13948620855808258, 0.25199615955352783, 0.09068343788385391, 0.0515279620885849, 0.46475499868392944, 0.042135339230298996, 0.22639097273349762, 0.22956779599189758, 0.011432219296693802, 0.05944925919175148, 0.09068664908409119, 0.18519353866577148, 0.18934646248817444, 0.24780064821243286, 0.3296770751476288, 0.6118367910385132, 0.17319515347480774, 0.15902483463287354, 0.12879395484924316, 0.41205742955207825, 0.4440748691558838, 0.06662964075803757, 0.13820138573646545, 0.13906261324882507, 0.2780062258243561, 0.484115868806839, 0.5969136357307434, 0.0766056552529335, 0.2496536821126938, 0.5116806626319885, 0.047871559858322144, 0.2760796546936035, 0.3117552101612091, 0.6801643967628479, 0.3721432685852051, 0.46947622299194336, 0.17273133993148804, 0.23732075095176697, 0.1866517812013626, 0.7481023073196411, 0.30957964062690735, 0.788384199142456, 0.06140279769897461, 0.6521003246307373, 0.295620858669281, 0.023604944348335266, 0.5318425893783569, 0.4817579686641693, 0.922904372215271, 0.6673303842544556]\n",
      "[0.3112179636955261, 0.31428802013397217, 0.010331135243177414, 0.04377473145723343, 0.19037415087223053, 0.34639739990234375, 0.308749794960022, 0.22929321229457855, 0.4253363013267517, 0.23604081571102142, 0.3492026925086975, 0.165953129529953, 0.027314599603414536, 0.15846864879131317, 0.1671934276819229, 0.4391043186187744, 0.23596540093421936, 0.2640325129032135, 0.23421618342399597, 0.20086365938186646, 0.4662027359008789, 0.20450977981090546, 0.47930169105529785, 0.1841420978307724, 0.09398523718118668, 0.23971809446811676, 0.1362161785364151, 0.23487967252731323, 0.2510848939418793, 0.2793945074081421, 0.23735393583774567, 0.026974637061357498, 0.1206459179520607, 0.13932393491268158, 0.04985677823424339, 0.1182432696223259, 0.015951678156852722, 0.42983877658843994, 0.009477691724896431, 0.420769065618515, 0.3335651755332947, 0.21629054844379425, 0.27796611189842224, 0.4486266076564789, 0.4396617114543915, 0.24252787232398987, 0.12920184433460236, 0.1750420182943344, 0.16742907464504242, 0.09273364394903183, 0.09619449824094772, 0.126409113407135, 0.046828363090753555, 0.026052268221974373, 0.23358075320720673, 0.022366415709257126, 0.11979038268327713, 0.12915512919425964, 0.028791971504688263, 0.10430705547332764, 0.06616783887147903, 0.09835651516914368, 0.12988504767417908, 0.13402239978313446, 0.23680710792541504, 0.356132835149765, 0.17645281553268433, 0.12498964369297028, 0.06673753261566162, 0.2529548406600952, 0.2455558478832245, 0.045766815543174744, 0.1103912740945816, 0.0860501080751419, 0.14896844327449799, 0.250938355922699, 0.2997486889362335, 0.042182184755802155, 0.17569082975387573, 0.3939993977546692, 0.026886800304055214, 0.22927509248256683, 0.1601981669664383, 0.4901812672615051, 0.333452433347702, 0.23708170652389526, 0.10123220831155777, 0.1230897307395935, 0.09858833998441696, 0.37846842408180237, 0.22382333874702454, 0.406906396150589, 0.22584369778633118, 0.33414798974990845, 0.1866014152765274, 0.01745009608566761, 0.2749154567718506, 0.24329686164855957, 0.46702250838279724, 0.33934667706489563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "# dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode, version='v2')\n",
    "correct = 0\n",
    "total = 0\n",
    "orig_confidences = []\n",
    "adv_confidences = []\n",
    "for d in tqdm(dataset):\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    image_id = original_graph.image_id\n",
    "\n",
    "    # load image\n",
    "    image = PIL.Image.open(image_dir + str(image_id) + \".jpg\").convert(\"RGB\")\n",
    "    image = prepocess_function(image).unsqueeze(0).to(device)\n",
    "\n",
    "    g_attr_confidences = []\n",
    "    def get_attr_confidence(g):\n",
    "        for node in g.nodes:\n",
    "            text_embd_obj = text_embeddings[g.nodes[node]['name']].to(device)\n",
    "            full_text_clip_embd = torch.cat((text_embd_obj, text_embd_obj), dim=0).reshape(1,-1)\n",
    "            obj_label = torch.tensor(obj_classes[g.nodes[node]['name']])\n",
    "            rel, obj1, obj2, attr1, attr2 = model(image, full_text_clip_embd.unsqueeze(0))\n",
    "            attr = torch.sigmoid((attr1+attr2)/2)\n",
    "            attr_labels = [attr_classes[attr] for attr in g.nodes[node]['attributes']]\n",
    "            for attr_label in attr_labels:\n",
    "                g_attr_confidences.append(attr[0][attr_label].item())\n",
    "            # text_embd_obj = text_embeddings[g.nodes[node]['name']].to(device)\n",
    "            # full_text_clip_embd = torch.cat((text_embd_obj, text_embd_obj), dim=0).reshape(1,-1)\n",
    "            # obj_label = torch.tensor(obj_classes[g.nodes[node]['name']])\n",
    "            # rel, obj1, obj2, attr1, attr2 = model(image.unsqueeze(0), full_text_clip_embd.unsqueeze(0))\n",
    "            # # mean of attr1 and attr2\n",
    "            # attr = (attr1 + attr2) / 2\n",
    "            # attr = torch.sigmoid(attr)\n",
    "            # attr_labels = [attr_classes[attr] for attr in g.nodes[node]['attributes']]\n",
    "            # for attr_label in attr_labels:\n",
    "            #     g_attr_confidences.append(attr[0][attr_label].item())\n",
    "        if len(g_attr_confidences) > 0:\n",
    "            attr_confidence = torch.mean(torch.tensor(g_attr_confidences)).cpu().item()\n",
    "        else:\n",
    "            assert False, \"no attributes found\"\n",
    "        return attr_confidence\n",
    "    \n",
    "    original_attr_confidence = get_attr_confidence(original_graph)\n",
    "    orig_confidences.append(original_attr_confidence)\n",
    "    adv_attr_confidence = get_attr_confidence(adv_graph)\n",
    "    adv_confidences.append(adv_attr_confidence)\n",
    "\n",
    "    if original_attr_confidence >= adv_attr_confidence:\n",
    "        correct += 1\n",
    "    else:\n",
    "        pass\n",
    "        # rel_name_dict = dataset_orig.rel_classes # a dict with the relationship names as keys and the ids as values\n",
    "        # rel_name = [key for key, value in rel_name_dict.items() if value == rel_label][0]\n",
    "        # rel_name_adv = [key for key, value in rel_name_dict.items() if value == rel_label_adv][0]\n",
    "        # print(\"orig rel:\", rel_name, \"adv rel:\", rel_name_adv, \"orig conf:\", original_rel_confidence, \"adv conf:\", adversarial_rel_confidence)\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)\n",
    "print(orig_confidences)\n",
    "print(adv_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9 total: 100 correct: 90\n",
      "[0.5067164301872253, 0.5126970410346985, 0.014940381050109863, 0.05342324823141098, 0.3533254861831665, 0.5326694250106812, 0.5962003469467163, 0.43216264247894287, 0.8438830375671387, 0.19870735704898834, 0.40836259722709656, 0.30747610330581665, 0.01407160609960556, 0.30224448442459106, 0.3189970552921295, 0.8700499534606934, 0.4589890241622925, 0.4802042245864868, 0.4660971760749817, 0.39960646629333496, 0.8092999458312988, 0.13675600290298462, 0.9554212689399719, 0.3491666913032532, 0.18109509348869324, 0.4731655716896057, 0.268600195646286, 0.4498292803764343, 0.272714227437973, 0.15753062069416046, 0.45829349756240845, 0.037026479840278625, 0.2376909703016281, 0.2652658224105835, 0.09565316140651703, 0.22291754186153412, 0.010786313563585281, 0.8500231504440308, 0.0007952179294079542, 0.8031555414199829, 0.6283320188522339, 0.42809051275253296, 0.5244889855384827, 0.8952550292015076, 0.632116973400116, 0.34471777081489563, 0.2500043213367462, 0.3405390977859497, 0.21550188958644867, 0.18403948843479156, 0.13948605954647064, 0.2519960403442383, 0.09068351238965988, 0.05152794346213341, 0.4647546708583832, 0.04213545471429825, 0.22639109194278717, 0.2295680046081543, 0.01143223512917757, 0.05944925919175148, 0.09068666398525238, 0.18519362807273865, 0.1893465220928192, 0.24780091643333435, 0.3296767771244049, 0.6118370294570923, 0.17319510877132416, 0.15902480483055115, 0.12879404425621033, 0.4120576083660126, 0.4440750181674957, 0.06662973761558533, 0.13820135593414307, 0.1390625536441803, 0.2780061364173889, 0.48411592841148376, 0.5969135165214539, 0.07660570740699768, 0.24965405464172363, 0.5116806030273438, 0.04787161573767662, 0.2760797142982483, 0.31175509095191956, 0.6801642179489136, 0.3721431493759155, 0.46947622299194336, 0.17273126542568207, 0.23732051253318787, 0.18665146827697754, 0.7481018304824829, 0.3095797896385193, 0.7883843183517456, 0.061402734369039536, 0.6520997285842896, 0.29562070965766907, 0.02360496297478676, 0.5318425893783569, 0.48175811767578125, 0.922904372215271, 0.6673301458358765]\n",
      "[0.3112179636955261, 0.31428787112236023, 0.010331135243177414, 0.04377486929297447, 0.1903742104768753, 0.3463975191116333, 0.3087497055530548, 0.22929315268993378, 0.4253363609313965, 0.23604071140289307, 0.34920263290405273, 0.1659531146287918, 0.027314580976963043, 0.15846861898899078, 0.16719353199005127, 0.43910443782806396, 0.23596546053886414, 0.2640325427055359, 0.23421618342399597, 0.20086362957954407, 0.4662027657032013, 0.20450961589813232, 0.47930169105529785, 0.18414215743541718, 0.09398525208234787, 0.23971819877624512, 0.13621608912944794, 0.23487965762615204, 0.2510850131511688, 0.2793944478034973, 0.23735399544239044, 0.026974642649292946, 0.1206459030508995, 0.13932405412197113, 0.0498567633330822, 0.1182432472705841, 0.015951678156852722, 0.42983874678611755, 0.009477702900767326, 0.420769065618515, 0.33356520533561707, 0.2162904441356659, 0.2779662013053894, 0.44862663745880127, 0.43966180086135864, 0.24252799153327942, 0.1292017549276352, 0.1750417798757553, 0.16742907464504242, 0.09273354709148407, 0.09619444608688354, 0.12640905380249023, 0.04682840034365654, 0.026052257046103477, 0.2335805892944336, 0.022366473451256752, 0.1197904422879219, 0.1291552186012268, 0.028791964054107666, 0.10430701076984406, 0.06616781651973724, 0.09835656732320786, 0.12988510727882385, 0.134022518992424, 0.23680686950683594, 0.35613301396369934, 0.17645272612571716, 0.12498962134122849, 0.0667375698685646, 0.25295498967170715, 0.24555593729019165, 0.04576689749956131, 0.11039134114980698, 0.0860501080751419, 0.1489683985710144, 0.250938355922699, 0.29974862933158875, 0.042182207107543945, 0.1756909042596817, 0.39399921894073486, 0.0268868338316679, 0.22927524149417877, 0.16019809246063232, 0.4901813268661499, 0.33345234394073486, 0.23708170652389526, 0.10123217105865479, 0.12308961153030396, 0.09858818352222443, 0.37846818566322327, 0.2238234281539917, 0.40690645575523376, 0.22584374248981476, 0.33414769172668457, 0.18660132586956024, 0.017450090497732162, 0.2749154567718506, 0.24329692125320435, 0.46702250838279724, 0.3393465578556061]\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", correct/total, \"total:\", total, \"correct:\", correct)\n",
    "print(orig_confidences)\n",
    "print(adv_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
