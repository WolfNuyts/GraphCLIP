{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the json to networkx graphs format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# append parent directory to path\n",
    "sys.path.append(\"..\")\n",
    "from datasets.VG_graphs import get_realistic_graphs_dataset\n",
    "dataset = get_realistic_graphs_dataset('v1', 'attr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "mode = \"text_embeddings\"\n",
    "\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes, n_attr_classes, shallow=True, input_mode=\"text_embeddings\", obj_heads=False):\n",
    "    from open_clip.jt_ViT_RelClassifier_lightning import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, n_attr_classes, clip_model_type, clip_pretrained_dataset, shallow=shallow, mode=input_mode, with_object_heads=obj_heads)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu(min_mem=20000)\n",
    "    print(f\"Using device {device}\")\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text embeddings as input to the model.\n",
      "Using device cuda:7\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters', 'datamodule_hparams_name', 'datamodule_hyper_parameters'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-L-14' # 'ViT-L-14' #'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion2b_s32b_b82k' # 'laion2b_s32b_b82k' #'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "\n",
    "model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200, 100, obj_heads=True)\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-05/vision_transformer_12/best_rel_model.ckpt\")\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-09/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "# new two ViT/B32 models:\n",
    "# no attribute wighting\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-22/vision_transformer_8/best_rel_model.ckpt\", map_location=device)\n",
    "# attribute wighting\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-22/vision_transformer_7/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# new models with different learning rates\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-23/vision_transformer_4/best_rel_model.ckpt\", map_location=device)\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-23/vision_transformer_5/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# loaded = torch.load(\"/local/home/jthomm/GraphCLIP/experiments/2023-05-24/vision_transformer_0/best_rel_model.ckpt\", map_location=device)\n",
    "\n",
    "# Big ViT-L\n",
    "# loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-05-25/vision_transformer_6/best_rel_model.ckpt', map_location=torch.device('cpu'))\n",
    "# loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-05-27/vision_transformer_39/model_epoch-v8.ckpt', map_location=torch.device('cpu'))\n",
    "loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-06-24/vision_transformer_8/model_epoch-v5.ckpt', map_location= device)\n",
    "print(loaded.keys())\n",
    "model.load_state_dict(loaded['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_OBJECTS = ['man', 'person', 'window', 'tree', 'building', 'shirt', 'wall', 'woman', 'sign', 'sky', 'ground', 'grass', 'table', 'pole', 'head', 'light', 'water', 'car', 'hand', 'hair', 'people', 'leg', 'trees', 'clouds', 'ear', 'plate', 'leaves', 'fence', 'door', 'pants', 'eye', 'train', 'chair', 'floor', 'road', 'street', 'hat', 'snow', 'wheel', 'shadow', 'jacket', 'nose', 'boy', 'line', 'shoe', 'clock', 'sidewalk', 'boat', 'tail', 'cloud', 'handle', 'letter', 'girl', 'leaf', 'horse', 'bus', 'helmet', 'bird', 'giraffe', 'field', 'plane', 'flower', 'elephant', 'umbrella', 'dog', 'shorts', 'arm', 'zebra', 'face', 'windows', 'sheep', 'glass', 'bag', 'cow', 'bench', 'cat', 'food', 'bottle', 'rock', 'tile', 'kite', 'tire', 'post', 'number', 'stripe', 'surfboard', 'truck', 'logo', 'glasses', 'roof', 'skateboard', 'motorcycle', 'picture', 'flowers', 'bear', 'player', 'foot', 'bowl', 'mirror', 'background', 'pizza', 'bike', 'shoes', 'spot', 'tracks', 'pillow', 'shelf', 'cap', 'mouth', 'box', 'jeans', 'dirt', 'lights', 'legs', 'house', 'part', 'trunk', 'banana', 'top', 'plant', 'cup', 'counter', 'board', 'bed', 'wave', 'bush', 'ball', 'sink', 'button', 'lamp', 'beach', 'brick', 'flag', 'neck', 'sand', 'vase', 'writing', 'wing', 'paper', 'seat', 'lines', 'reflection', 'coat', 'child', 'toilet', 'laptop', 'airplane', 'letters', 'glove', 'vehicle', 'phone', 'book', 'branch', 'sunglasses', 'edge', 'cake', 'desk', 'rocks', 'frisbee', 'tie', 'tower', 'animal', 'hill', 'mountain', 'headlight', 'ceiling', 'cabinet', 'eyes', 'stripes', 'wheels', 'lady', 'ocean', 'racket', 'container', 'skier', 'keyboard', 'towel', 'frame', 'windshield', 'hands', 'back', 'track', 'bat', 'finger', 'pot', 'orange', 'fork', 'waves', 'design', 'feet', 'basket', 'fruit', 'broccoli', 'engine', 'guy', 'knife', 'couch', 'railing', 'collar', 'cars']\n",
    "FILTERED_RELATIONSHIPS = ['on', 'has', 'in', 'of', 'wearing', 'with', 'behind', 'holding', 'on a', 'near', 'on top of', 'next to', 'has a', 'under', 'of a', 'by', 'above', 'wears', 'in front of', 'sitting on', 'on side of', 'attached to', 'wearing a', 'in a', 'over', 'are on', 'at', 'for', 'around', 'beside', 'standing on', 'riding', 'standing in', 'inside', 'have', 'hanging on', 'walking on', 'on front of', 'are in', 'hanging from', 'carrying', 'holds', 'covering', 'belonging to', 'between', 'along', 'eating', 'and', 'sitting in', 'watching', 'below', 'painted on', 'laying on', 'against', 'playing', 'from', 'inside of', 'looking at', 'with a', 'parked on', 'to', 'has an', 'made of', 'covered in', 'mounted on', 'says', 'growing on', 'across', 'part of', 'on back of', 'flying in', 'outside', 'lying on', 'worn by', 'walking in', 'sitting at', 'printed on', 'underneath', 'crossing', 'beneath', 'full of', 'using', 'filled with', 'hanging in', 'covered with', 'built into', 'standing next to', 'adorning', 'a', 'in middle of', 'flying', 'supporting', 'touching', 'next', 'swinging', 'pulling', 'growing in', 'sitting on top of', 'standing', 'lying on top of']\n",
    "FILTERED_ATTRIBUTES = ['white', 'black', 'blue', 'green', 'red', 'brown', 'yellow', 'small', 'large', 'wooden', 'gray', 'silver', 'metal', 'orange', 'grey', 'tall', 'long', 'dark', 'pink', 'clear', 'standing', 'round', 'tan', 'glass', 'here', 'wood', 'open', 'purple', 'big', 'short', 'plastic', 'parked', 'sitting', 'walking', 'striped', 'brick', 'young', 'gold', 'old', 'hanging', 'empty', 'on', 'bright', 'concrete', 'cloudy', 'colorful', 'one', 'beige', 'bare', 'wet', 'light', 'square', 'little', 'closed', 'stone', 'blonde', 'shiny', 'thin', 'dirty', 'flying', 'smiling', 'painted', 'thick', 'part', 'sliced', 'playing', 'tennis', 'calm', 'leather', 'distant', 'rectangular', 'looking', 'grassy', 'dry', 'light brown', 'cement', 'leafy', 'wearing', 'tiled', \"man's\", 'light blue', 'baseball', 'cooked', 'pictured', 'curved', 'decorative', 'dead', 'eating', 'paper', 'paved', 'fluffy', 'lit', 'back', 'framed', 'plaid', 'dirt', 'watching', 'colored', 'stuffed', 'circular']\n",
    "rel_classes = {rel:i for i,rel in enumerate(FILTERED_RELATIONSHIPS)}\n",
    "obj_classes = {obj:i for i,obj in enumerate(FILTERED_OBJECTS)}\n",
    "attr_classes = {attr:i for i,attr in enumerate(FILTERED_ATTRIBUTES)}\n",
    "import os\n",
    "obj_embeddings = torch.load(os.path.join('/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed', 'filtered_object_label_embeddings.pt'), map_location=device)\n",
    "text_embeddings = {obj:torch.tensor(obj_embeddings[i]) for i,obj in enumerate(FILTERED_OBJECTS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.86\n",
      "[0.45541584491729736, 0.48297837376594543, 0.06513210386037827, 0.11103682219982147, 0.4035613536834717, 0.4509696960449219, 0.7852519154548645, 0.29990386962890625, 0.3324749767780304, 0.254874587059021, 0.27927324175834656, 0.3090481460094452, 0.10491643100976944, 0.2049470841884613, 0.20878340303897858, 0.8286699652671814, 0.402504563331604, 0.4008106589317322, 0.49500948190689087, 0.4863755702972412, 0.5385722517967224, 0.026003647595643997, 0.9252387881278992, 0.09813953936100006, 0.03480200842022896, 0.44149070978164673, 0.29218316078186035, 0.45412227511405945, 0.38451242446899414, 0.48835867643356323, 0.5550113320350647, 0.15155284106731415, 0.4180411696434021, 0.4373331665992737, 0.018537918105721474, 0.03622426837682724, 0.01255097147077322, 0.6252033114433289, 0.0017047952860593796, 0.5808531045913696, 0.44069939851760864, 0.19329248368740082, 0.6924962997436523, 0.8526638746261597, 0.5562286376953125, 0.43707287311553955, 0.38576191663742065, 0.4296140670776367, 0.2736067473888397, 0.20923560857772827, 0.07725125551223755, 0.2583554685115814, 0.14310301840305328, 0.06006065011024475, 0.3160994350910187, 0.09114336222410202, 0.06414443999528885, 0.13746009767055511, 0.03666505217552185, 0.11225400120019913, 0.05398203432559967, 0.14827150106430054, 0.24879150092601776, 0.10793675482273102, 0.34825196862220764, 0.6672792434692383, 0.06648296117782593, 0.09053511172533035, 0.3522164225578308, 0.5180463790893555, 0.04961039125919342, 0.0794752761721611, 0.15202797949314117, 0.123808354139328, 0.2320946902036667, 0.49764037132263184, 0.48555639386177063, 0.03547711670398712, 0.08817857503890991, 0.6006181240081787, 0.06869196146726608, 0.25157105922698975, 0.46602457761764526, 0.3275565505027771, 0.19995643198490143, 0.4744679033756256, 0.14187172055244446, 0.2895437479019165, 0.2611771821975708, 0.6208928227424622, 0.31020429730415344, 0.6963744163513184, 0.04705546796321869, 0.3990830183029175, 0.21929866075515747, 0.02030244842171669, 0.6493500471115112, 0.6349520087242126, 0.8266017436981201, 0.5270843505859375]\n",
      "[0.26298987865448, 0.27696508169174194, 0.05647944658994675, 0.07021664828062057, 0.27186641097068787, 0.26748228073120117, 0.4178979694843292, 0.15819396078586578, 0.1677989512681961, 0.2884005904197693, 0.22356383502483368, 0.18373773992061615, 0.12524791061878204, 0.11488883942365646, 0.11694829910993576, 0.4214584231376648, 0.21938002109527588, 0.20511247217655182, 0.2528776228427887, 0.24640439450740814, 0.32090240716934204, 0.04212426766753197, 0.46604278683662415, 0.0639813169836998, 0.023966677486896515, 0.22497186064720154, 0.1494082808494568, 0.2322688102722168, 0.24573636054992676, 0.3959580063819885, 0.28807541728019714, 0.08115652948617935, 0.2308991253376007, 0.24875934422016144, 0.009685251861810684, 0.042993444949388504, 0.009163573384284973, 0.325173020362854, 0.07590657472610474, 0.32042738795280457, 0.24968814849853516, 0.10112917423248291, 0.40364423394203186, 0.4271220862865448, 0.3604138493537903, 0.2839305102825165, 0.23534150421619415, 0.24881067872047424, 0.19442126154899597, 0.10602830350399017, 0.23043954372406006, 0.13169050216674805, 0.07437841594219208, 0.03034825064241886, 0.1588362753391266, 0.04607442021369934, 0.03643883764743805, 0.0776340439915657, 0.09040245413780212, 0.13047702610492706, 0.03661022335290909, 0.10494203120470047, 0.22191321849822998, 0.059334151446819305, 0.2265954315662384, 0.3528822660446167, 0.1035257950425148, 0.10152342915534973, 0.17889729142189026, 0.2715291380882263, 0.05209527164697647, 0.0454513281583786, 0.14075912535190582, 0.10512867569923401, 0.13541850447654724, 0.25739291310310364, 0.271105021238327, 0.019615311175584793, 0.13543720543384552, 0.5095553994178772, 0.03684864193201065, 0.24781019985675812, 0.23881667852401733, 0.2822473645210266, 0.19794109463691711, 0.2409839630126953, 0.07693248242139816, 0.16616278886795044, 0.14482475817203522, 0.35009539127349854, 0.27237603068351746, 0.36305826902389526, 0.1326448619365692, 0.21931712329387665, 0.2231130599975586, 0.013349550776183605, 0.3402964472770691, 0.3204008936882019, 0.4200390875339508, 0.2923595905303955]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "# dataset_orig, dataset_adv, list_to_iterate = get_realistic_graphs_dataset_ViT(prepocess_function, image_dir, mode=mode, version='v2')\n",
    "correct = 0\n",
    "total = 0\n",
    "orig_confidences = []\n",
    "adv_confidences = []\n",
    "for d in tqdm(dataset):\n",
    "    original_graph = d['original_graph']\n",
    "    adv_graph = d['adv_graph']\n",
    "    image_id = original_graph.image_id\n",
    "\n",
    "    # load image\n",
    "    image = PIL.Image.open(image_dir + str(image_id) + \".jpg\").convert(\"RGB\")\n",
    "    image = prepocess_function(image).unsqueeze(0).to(device)\n",
    "\n",
    "    g_attr_confidences = []\n",
    "    def get_attr_confidence(g):\n",
    "        for node in g.nodes:\n",
    "            text_embd_obj = text_embeddings[g.nodes[node]['name']].to(device)\n",
    "            full_text_clip_embd = torch.cat((text_embd_obj, text_embd_obj), dim=0).reshape(1,-1)\n",
    "            obj_label = torch.tensor(obj_classes[g.nodes[node]['name']])\n",
    "            rel, obj1, obj2, attr1, attr2 = model(image, full_text_clip_embd.unsqueeze(0))\n",
    "            attr = torch.sigmoid((attr1+attr2)/2)\n",
    "            attr_labels = [attr_classes[attr] for attr in g.nodes[node]['attributes']]\n",
    "            for attr_label in attr_labels:\n",
    "                g_attr_confidences.append(attr[0][attr_label].item())\n",
    "            # text_embd_obj = text_embeddings[g.nodes[node]['name']].to(device)\n",
    "            # full_text_clip_embd = torch.cat((text_embd_obj, text_embd_obj), dim=0).reshape(1,-1)\n",
    "            # obj_label = torch.tensor(obj_classes[g.nodes[node]['name']])\n",
    "            # rel, obj1, obj2, attr1, attr2 = model(image.unsqueeze(0), full_text_clip_embd.unsqueeze(0))\n",
    "            # # mean of attr1 and attr2\n",
    "            # attr = (attr1 + attr2) / 2\n",
    "            # attr = torch.sigmoid(attr)\n",
    "            # attr_labels = [attr_classes[attr] for attr in g.nodes[node]['attributes']]\n",
    "            # for attr_label in attr_labels:\n",
    "            #     g_attr_confidences.append(attr[0][attr_label].item())\n",
    "        if len(g_attr_confidences) > 0:\n",
    "            attr_confidence = torch.mean(torch.tensor(g_attr_confidences)).cpu().item()\n",
    "        else:\n",
    "            assert False, \"no attributes found\"\n",
    "        return attr_confidence\n",
    "    \n",
    "    original_attr_confidence = get_attr_confidence(original_graph)\n",
    "    orig_confidences.append(original_attr_confidence)\n",
    "    adv_attr_confidence = get_attr_confidence(adv_graph)\n",
    "    adv_confidences.append(adv_attr_confidence)\n",
    "\n",
    "    if original_attr_confidence >= adv_attr_confidence:\n",
    "        correct += 1\n",
    "    else:\n",
    "        pass\n",
    "        # rel_name_dict = dataset_orig.rel_classes # a dict with the relationship names as keys and the ids as values\n",
    "        # rel_name = [key for key, value in rel_name_dict.items() if value == rel_label][0]\n",
    "        # rel_name_adv = [key for key, value in rel_name_dict.items() if value == rel_label_adv][0]\n",
    "        # print(\"orig rel:\", rel_name, \"adv rel:\", rel_name_adv, \"orig conf:\", original_rel_confidence, \"adv conf:\", adversarial_rel_confidence)\n",
    "    total += 1\n",
    "    # print(\"original relationship confidence:\", original_rel_confidence)\n",
    "    # print(\"adversarial relationship confidence:\", adversarial_rel_confidence)\n",
    "    # break\n",
    "print(\"accuracy:\", correct/total)\n",
    "print(orig_confidences)\n",
    "print(adv_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.86 total: 100 correct: 86\n",
      "[0.45541584491729736, 0.48297837376594543, 0.06513210386037827, 0.11103682219982147, 0.4035613536834717, 0.4509696960449219, 0.7852519154548645, 0.29990386962890625, 0.3324749767780304, 0.254874587059021, 0.27927324175834656, 0.3090481460094452, 0.10491643100976944, 0.2049470841884613, 0.20878340303897858, 0.8286699652671814, 0.402504563331604, 0.4008106589317322, 0.49500948190689087, 0.4863755702972412, 0.5385722517967224, 0.026003647595643997, 0.9252387881278992, 0.09813953936100006, 0.03480200842022896, 0.44149070978164673, 0.29218316078186035, 0.45412227511405945, 0.38451242446899414, 0.48835867643356323, 0.5550113320350647, 0.15155284106731415, 0.4180411696434021, 0.4373331665992737, 0.018537918105721474, 0.03622426837682724, 0.01255097147077322, 0.6252033114433289, 0.0017047952860593796, 0.5808531045913696, 0.44069939851760864, 0.19329248368740082, 0.6924962997436523, 0.8526638746261597, 0.5562286376953125, 0.43707287311553955, 0.38576191663742065, 0.4296140670776367, 0.2736067473888397, 0.20923560857772827, 0.07725125551223755, 0.2583554685115814, 0.14310301840305328, 0.06006065011024475, 0.3160994350910187, 0.09114336222410202, 0.06414443999528885, 0.13746009767055511, 0.03666505217552185, 0.11225400120019913, 0.05398203432559967, 0.14827150106430054, 0.24879150092601776, 0.10793675482273102, 0.34825196862220764, 0.6672792434692383, 0.06648296117782593, 0.09053511172533035, 0.3522164225578308, 0.5180463790893555, 0.04961039125919342, 0.0794752761721611, 0.15202797949314117, 0.123808354139328, 0.2320946902036667, 0.49764037132263184, 0.48555639386177063, 0.03547711670398712, 0.08817857503890991, 0.6006181240081787, 0.06869196146726608, 0.25157105922698975, 0.46602457761764526, 0.3275565505027771, 0.19995643198490143, 0.4744679033756256, 0.14187172055244446, 0.2895437479019165, 0.2611771821975708, 0.6208928227424622, 0.31020429730415344, 0.6963744163513184, 0.04705546796321869, 0.3990830183029175, 0.21929866075515747, 0.02030244842171669, 0.6493500471115112, 0.6349520087242126, 0.8266017436981201, 0.5270843505859375]\n",
      "[0.26298987865448, 0.27696508169174194, 0.05647944658994675, 0.07021664828062057, 0.27186641097068787, 0.26748228073120117, 0.4178979694843292, 0.15819396078586578, 0.1677989512681961, 0.2884005904197693, 0.22356383502483368, 0.18373773992061615, 0.12524791061878204, 0.11488883942365646, 0.11694829910993576, 0.4214584231376648, 0.21938002109527588, 0.20511247217655182, 0.2528776228427887, 0.24640439450740814, 0.32090240716934204, 0.04212426766753197, 0.46604278683662415, 0.0639813169836998, 0.023966677486896515, 0.22497186064720154, 0.1494082808494568, 0.2322688102722168, 0.24573636054992676, 0.3959580063819885, 0.28807541728019714, 0.08115652948617935, 0.2308991253376007, 0.24875934422016144, 0.009685251861810684, 0.042993444949388504, 0.009163573384284973, 0.325173020362854, 0.07590657472610474, 0.32042738795280457, 0.24968814849853516, 0.10112917423248291, 0.40364423394203186, 0.4271220862865448, 0.3604138493537903, 0.2839305102825165, 0.23534150421619415, 0.24881067872047424, 0.19442126154899597, 0.10602830350399017, 0.23043954372406006, 0.13169050216674805, 0.07437841594219208, 0.03034825064241886, 0.1588362753391266, 0.04607442021369934, 0.03643883764743805, 0.0776340439915657, 0.09040245413780212, 0.13047702610492706, 0.03661022335290909, 0.10494203120470047, 0.22191321849822998, 0.059334151446819305, 0.2265954315662384, 0.3528822660446167, 0.1035257950425148, 0.10152342915534973, 0.17889729142189026, 0.2715291380882263, 0.05209527164697647, 0.0454513281583786, 0.14075912535190582, 0.10512867569923401, 0.13541850447654724, 0.25739291310310364, 0.271105021238327, 0.019615311175584793, 0.13543720543384552, 0.5095553994178772, 0.03684864193201065, 0.24781019985675812, 0.23881667852401733, 0.2822473645210266, 0.19794109463691711, 0.2409839630126953, 0.07693248242139816, 0.16616278886795044, 0.14482475817203522, 0.35009539127349854, 0.27237603068351746, 0.36305826902389526, 0.1326448619365692, 0.21931712329387665, 0.2231130599975586, 0.013349550776183605, 0.3402964472770691, 0.3204008936882019, 0.4200390875339508, 0.2923595905303955]\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", correct/total, \"total:\", total, \"correct:\", correct)\n",
    "print(orig_confidences)\n",
    "print(adv_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
