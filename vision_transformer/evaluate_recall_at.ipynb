{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jt_training import get_dataloader, train_one_epoch, evaluate, get_free_gpu\n",
    "\n",
    "\n",
    "def load_model(clip_model_type, clip_pretrained_dataset, n_rel_classes, n_obj_classes, n_attr_classes, shallow=True, input_mode=\"text_embeddings\", with_object_heads=False):\n",
    "    from open_clip.jt_ViT_RelClassifier_lightning import ViT_RelClassifier\n",
    "    model = ViT_RelClassifier(n_rel_classes, n_obj_classes, n_attr_classes, clip_model_type, clip_pretrained_dataset, shallow=shallow, mode=input_mode, with_object_heads=with_object_heads)\n",
    "    prepocess_function = model.preprocess\n",
    "    device = get_free_gpu(min_mem=20000)\n",
    "    print(f\"Using device {device}\")\n",
    "    model.to(device)\n",
    "    return model, prepocess_function, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text embeddings as input to the model.\n",
      "Using device cuda:2\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters', 'datamodule_hparams_name', 'datamodule_hyper_parameters'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "clip_model_type = 'ViT-L-14' # 'ViT-L-14' #'ViT-B/32'\n",
    "clip_pretrained_dataset = 'laion2b_s32b_b82k' # 'laion2b_s32b_b82k' #'laion400m_e32'\n",
    "image_dir = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/raw/VG/\"\n",
    "metadata_path = \"/local/home/jthomm/GraphCLIP/datasets/visual_genome/processed/\"\n",
    "\n",
    "\n",
    "model, prepocess_function, device = load_model(clip_model_type, clip_pretrained_dataset, 100, 200, 100, with_object_heads=True)\n",
    "\n",
    "loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-05-27/vision_transformer_39/model_epoch-v29.ckpt', map_location=device)\n",
    "# loaded = torch.load('/local/home/jthomm/GraphCLIP/experiments/2023-06-24/vision_transformer_8/model_epoch-v9.ckpt', map_location= device)\n",
    "print(loaded.keys())\n",
    "model.load_state_dict(loaded['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered graphs...\n",
      "100\n",
      "Done loading filtered graphs.\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered relationships loaded from file\n",
      "Filtered objects loaded from file\n",
      "Filtered attributes loaded from file\n",
      "Filtered objects loaded from file\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_val, graphs_train, graphs_val = get_dataloader(prepocess_function,metadata_path,image_dir, testing_only=False, get_pure_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean recall at 50: 0.6089737843343489, Mean recall at 100: 0.609061925747666, Mean recall at 50 k100: 0.954872754106252, Mean recall at 100 k100: 1.013989807486511:  62%|██████▏   | 11965/19419 [53:24<33:16,  3.73it/s]     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m obj2_name \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mnodes[edge[\u001b[39m1\u001b[39m]][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m predicate \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medges[edge][\u001b[39m'\u001b[39m\u001b[39mpredicate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m image, full_text_clip_embd, rel_label, obj1_label, obj2_label, attr1_label, attr2_label, rel_mask, attr1_mask, attr2_mask \u001b[39m=\u001b[39m dataset_val\u001b[39m.\u001b[39;49mgetitem_from_id_edge(graph\u001b[39m.\u001b[39;49mimage_id, edge, \u001b[39m\"\u001b[39;49m\u001b[39mtext_embeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m rel, obj1, obj2, attr1, attr2 \u001b[39m=\u001b[39m model(image\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device), full_text_clip_embd\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     33\u001b[0m true_rel_confidence \u001b[39m=\u001b[39m rel[\u001b[39m0\u001b[39m][rel_label]\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/GraphCLIP/vision_transformer/jt_training/visual_genome_dataset.py:93\u001b[0m, in \u001b[0;36mCustomImageDataset.getitem_from_id_edge\u001b[0;34m(self, image_id, edge, mode)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetitem_from_id_edge\u001b[39m(\u001b[39mself\u001b[39m, image_id, edge, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbounding_boxes\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m     img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mimage_id\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img_path)\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m     95\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/PIL/Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[1;32m    874\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[1;32m    875\u001b[0m ):\n\u001b[1;32m    876\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    923\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    924\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    925\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/PIL/ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    259\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 260\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "# shuffle not in place\n",
    "import random\n",
    "graphs_val_list = list(graphs_val)\n",
    "random.shuffle(graphs_val_list)\n",
    "\n",
    "\n",
    "first_n = 10000000\n",
    "len(graphs_val)\n",
    "print(len(graphs_val))\n",
    "mean_recall_at_50 = 0\n",
    "mean_recall_at_100 = 0\n",
    "mean_recall_at_50_k100 = 0\n",
    "mean_recall_at_100_k100 = 0\n",
    "t = trange(len(graphs_val), desc='Bar desc', leave=True)\n",
    "n_processed = 0\n",
    "for i in t:\n",
    "    graph = graphs_val_list[i]\n",
    "    if len(graph.edges) == 0:\n",
    "        continue\n",
    "    first_n -= 1\n",
    "    if first_n < 0:\n",
    "        break\n",
    "    guesses_k1 = []\n",
    "    true_rels = []\n",
    "    guesses_k100 = []\n",
    "    for edge in graph.edges:\n",
    "        obj1_name = graph.nodes[edge[0]]['name']\n",
    "        obj2_name = graph.nodes[edge[1]]['name']\n",
    "        predicate = graph.edges[edge]['predicate']\n",
    "        image, full_text_clip_embd, rel_label, obj1_label, obj2_label, attr1_label, attr2_label, rel_mask, attr1_mask, attr2_mask = dataset_val.getitem_from_id_edge(graph.image_id, edge, \"text_embeddings\")\n",
    "        rel, obj1, obj2, attr1, attr2 = model(image.unsqueeze(0).to(device), full_text_clip_embd.unsqueeze(0).to(device))\n",
    "        true_rel_confidence = rel[0][rel_label].item()\n",
    "        true_rel = rel_label\n",
    "        pred_rel_confidence = rel[0].max().item()\n",
    "        pred_rel = rel[0].argmax().item()\n",
    "        true_rels.append((obj1_name, true_rel, obj2_name))\n",
    "        guesses_k1.append((true_rel_confidence, obj1_name, pred_rel, obj2_name))\n",
    "        guesses_k100 += [(rel[0][i].item(), obj1_name, i, obj2_name) for i in range(100)]\n",
    "\n",
    "    guesses_k1.sort(key=lambda x: x[0], reverse=True)\n",
    "    guesses_k100.sort(key=lambda x: x[0], reverse=True)\n",
    "    # calculate recall at 50 and 100\n",
    "    recall_at_50 = 0\n",
    "    recall_at_100 = 0\n",
    "    recall_at_50_k100 = 0\n",
    "    recall_at_100_k100 = 0\n",
    "    for i in range(50):\n",
    "        if i>=len(guesses_k1):\n",
    "            break\n",
    "        if guesses_k1[i][1:] in true_rels:\n",
    "            recall_at_50 += 1\n",
    "    for i in range(100):\n",
    "        if i>=len(guesses_k1):\n",
    "            break\n",
    "        if guesses_k1[i][1:] in true_rels:\n",
    "            recall_at_100 += 1\n",
    "    for i in range(50):\n",
    "        if i>=len(guesses_k100):\n",
    "            break\n",
    "        if guesses_k100[i][1:] in true_rels:\n",
    "            recall_at_50_k100 += 1\n",
    "    for i in range(100):\n",
    "        if i>=len(guesses_k100):\n",
    "            break\n",
    "        if guesses_k100[i][1:] in true_rels:\n",
    "            recall_at_100_k100 += 1\n",
    "    mean_recall_at_50 += recall_at_50/len(true_rels)\n",
    "    mean_recall_at_100 += recall_at_100/len(true_rels)\n",
    "    mean_recall_at_50_k100 += recall_at_50_k100/len(true_rels)\n",
    "    mean_recall_at_100_k100 += recall_at_100_k100/len(true_rels)\n",
    "    n_processed += 1\n",
    "\n",
    "    t.set_description(f\"Mean recall at 50: {mean_recall_at_50/n_processed}, Mean recall at 100: {mean_recall_at_100/n_processed}, Mean recall at 50 k100: {mean_recall_at_50_k100/n_processed}, Mean recall at 100 k100: {mean_recall_at_100_k100/n_processed}\")\n",
    "    \n",
    "\n",
    "print(f\"Mean recall at 50: {mean_recall_at_50/100}\")\n",
    "print(f\"Mean recall at 100: {mean_recall_at_100/100}\")\n",
    "print(f\"Mean recall at 50 k100: {mean_recall_at_50_k100/100}\")\n",
    "print(f\"Mean recall at 100 k100: {mean_recall_at_100_k100/100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean recall at 50: 0.5662773817134613, Mean recall at 100: 0.5662773817134613, Mean recall at 50 k100: 0.9311437132423748, Mean recall at 100 k100: 0.9903194200546717, Mean acc: 0.596767333049766:   2%|▏         | 295/19419 [01:19<1:25:29,  3.73it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m predicate \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medges[edge][\u001b[39m'\u001b[39m\u001b[39mpredicate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m image, full_text_clip_embd, rel_label, obj1_label, obj2_label, attr1_label, attr2_label, rel_mask, attr1_mask, attr2_mask \u001b[39m=\u001b[39m dataset_val\u001b[39m.\u001b[39mgetitem_from_id_edge(graph\u001b[39m.\u001b[39mimage_id, edge, \u001b[39m\"\u001b[39m\u001b[39mtext_embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m rel, obj1, obj2, attr1, attr2 \u001b[39m=\u001b[39m model(image\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device), full_text_clip_embd\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     35\u001b[0m true_rel_confidence \u001b[39m=\u001b[39m rel[\u001b[39m0\u001b[39m][rel_label]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m true_rel \u001b[39m=\u001b[39m rel_label\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GraphCLIP/vision_transformer/open_clip/jt_ViT_RelClassifier_lightning.py:112\u001b[0m, in \u001b[0;36mViT_RelClassifier.forward\u001b[0;34m(self, x, bounding_boxes)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, bounding_boxes\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     bounding_boxes_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_conv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounding_boxes_map(bounding_boxes))\n\u001b[0;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mViT(x, bounding_boxes_embedding)\n\u001b[1;32m    113\u001b[0m     rel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrel_classifier(x)\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_object_heads:\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GraphCLIP/vision_transformer/open_clip/transformer.py:488\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x, bounding_boxes_embedding)\u001b[0m\n\u001b[1;32m    485\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    487\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    489\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_pool \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GraphCLIP/vision_transformer/open_clip/transformer.py:320\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    318\u001b[0m         x \u001b[39m=\u001b[39m checkpoint(r, x, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    319\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         x \u001b[39m=\u001b[39m r(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GraphCLIP/vision_transformer/open_clip/transformer.py:243\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m k_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(k_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m k_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q_x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(q_x), k_x\u001b[39m=\u001b[39;49mk_x, v_x\u001b[39m=\u001b[39;49mv_x, attn_mask\u001b[39m=\u001b[39;49mattn_mask))\n\u001b[1;32m    244\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))\n\u001b[1;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/GraphCLIP/vision_transformer/open_clip/transformer.py:229\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    226\u001b[0m v_x \u001b[39m=\u001b[39m v_x \u001b[39mif\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m q_x\n\u001b[1;32m    228\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(q_x\u001b[39m.\u001b[39mdtype) \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    230\u001b[0m     q_x, k_x, v_x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m    231\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/modules/activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/jtpython2/lib/python3.10/site-packages/torch/nn/functional.py:5337\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5334\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5335\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m-> 5337\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n\u001b[1;32m   5338\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[1;32m   5339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5340\u001b[0m     \u001b[39m# squeeze the output if input was unbatched\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "# shuffle not in place\n",
    "import random\n",
    "graphs_val_list = list(graphs_val)\n",
    "random.shuffle(graphs_val_list)\n",
    "\n",
    "\n",
    "first_n = 10000000\n",
    "len(graphs_val)\n",
    "print(len(graphs_val))\n",
    "mean_recall_at_50 = 0\n",
    "mean_recall_at_100 = 0\n",
    "mean_recall_at_50_k100 = 0\n",
    "mean_recall_at_100_k100 = 0\n",
    "mean_acc = 0\n",
    "acc_steps = 0\n",
    "t = trange(len(graphs_val), desc='Bar desc', leave=True)\n",
    "n_processed = 0\n",
    "for i in t:\n",
    "    graph = graphs_val_list[i]\n",
    "    if len(graph.edges) == 0:\n",
    "        continue\n",
    "    first_n -= 1\n",
    "    if first_n < 0:\n",
    "        break\n",
    "    guesses_k1 = []\n",
    "    true_rels = []\n",
    "    guesses_k100 = []\n",
    "    for edge in graph.edges:\n",
    "        obj1_name = graph.nodes[edge[0]]['name']\n",
    "        obj2_name = graph.nodes[edge[1]]['name']\n",
    "        predicate = graph.edges[edge]['predicate']\n",
    "        image, full_text_clip_embd, rel_label, obj1_label, obj2_label, attr1_label, attr2_label, rel_mask, attr1_mask, attr2_mask = dataset_val.getitem_from_id_edge(graph.image_id, edge, \"text_embeddings\")\n",
    "        rel, obj1, obj2, attr1, attr2 = model(image.unsqueeze(0).to(device), full_text_clip_embd.unsqueeze(0).to(device))\n",
    "        true_rel_confidence = rel[0][rel_label].item()\n",
    "        true_rel = rel_label\n",
    "        pred_rel_confidence = rel[0].max().item()\n",
    "        pred_rel = rel[0].argmax().item()\n",
    "        if true_rel == pred_rel:\n",
    "            mean_acc += 1\n",
    "        acc_steps += 1\n",
    "        true_rels.append((obj1_name, true_rel, obj2_name))\n",
    "        guesses_k1.append((true_rel_confidence, obj1_name, pred_rel, obj2_name))\n",
    "        guesses_k100 += [(rel[0][i].item(), obj1_name, i, obj2_name) for i in range(100)]\n",
    "\n",
    "    guesses_k1.sort(key=lambda x: x[0], reverse=True)\n",
    "    guesses_k100.sort(key=lambda x: x[0], reverse=True)\n",
    "    # calculate recall at 50 and 100\n",
    "    recall_at_50 = 0\n",
    "    recall_at_100 = 0\n",
    "    recall_at_50_k100 = 0\n",
    "    recall_at_100_k100 = 0\n",
    "    for i in range(50):\n",
    "        if i>=len(guesses_k1):\n",
    "            break\n",
    "        if guesses_k1[i][1:] in true_rels:\n",
    "            recall_at_50 += 1\n",
    "    for i in range(100):\n",
    "        if i>=len(guesses_k1):\n",
    "            break\n",
    "        if guesses_k1[i][1:] in true_rels:\n",
    "            recall_at_100 += 1\n",
    "    for i in range(50):\n",
    "        if i>=len(guesses_k100):\n",
    "            break\n",
    "        if guesses_k100[i][1:] in true_rels:\n",
    "            recall_at_50_k100 += 1\n",
    "    for i in range(100):\n",
    "        if i>=len(guesses_k100):\n",
    "            break\n",
    "        if guesses_k100[i][1:] in true_rels:\n",
    "            recall_at_100_k100 += 1\n",
    "    mean_recall_at_50 += recall_at_50/len(true_rels)\n",
    "    mean_recall_at_100 += recall_at_100/len(true_rels)\n",
    "    mean_recall_at_50_k100 += recall_at_50_k100/len(true_rels)\n",
    "    mean_recall_at_100_k100 += recall_at_100_k100/len(true_rels)\n",
    "    n_processed += 1\n",
    "\n",
    "    t.set_description(f\"Mean recall at 50: {mean_recall_at_50/n_processed}, Mean recall at 100: {mean_recall_at_100/n_processed}, Mean recall at 50 k100: {mean_recall_at_50_k100/n_processed}, Mean recall at 100 k100: {mean_recall_at_100_k100/n_processed}, Mean acc: {mean_acc/acc_steps}\")\n",
    "    \n",
    "\n",
    "print(f\"Mean recall at 50: {mean_recall_at_50/100}\")\n",
    "print(f\"Mean recall at 100: {mean_recall_at_100/100}\")\n",
    "print(f\"Mean recall at 50 k100: {mean_recall_at_50_k100/100}\")\n",
    "print(f\"Mean recall at 100 k100: {mean_recall_at_100_k100/100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaltestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
