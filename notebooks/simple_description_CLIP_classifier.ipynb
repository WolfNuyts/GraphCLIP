{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image as PIL_Image\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from torch.nn.functional import cosine_similarity as cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/local/home/stuff/visual-genome/'\n",
    "objects = json.load(open(DATA_PATH + 'objects.json'))\n",
    "image_ids = [o['image_id'] for o in objects]\n",
    "def get_image(image_id):\n",
    "    try:\n",
    "        image = PIL_Image.open(DATA_PATH + 'VG_100K/' + str(image_id) + '.jpg')\n",
    "    except:\n",
    "        image = PIL_Image.open(DATA_PATH + 'VG_100K_2/' + str(image_id) + '.jpg')\n",
    "    return image\n",
    "shuffled_image_ids = np.random.permutation(image_ids)\n",
    "del image_ids\n",
    "image_id_corpus = shuffled_image_ids[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded region descriptions\n",
      "loaded attributes\n",
      "loaded relationships\n"
     ]
    }
   ],
   "source": [
    "region_descriptions = json.load(open(DATA_PATH + 'region_descriptions.json'))\n",
    "print(\"loaded region descriptions\")\n",
    "attributes = json.load(open(DATA_PATH + 'attributes.json'))\n",
    "print(\"loaded attributes\")\n",
    "relationships = json.load(open(DATA_PATH + 'relationships.json'))\n",
    "print(\"loaded relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently the format is different than state in the website description\n",
    "\n",
    "# the attributes always contain the objects\n",
    "# attributes[0]['attributes'][0]['attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes2 = {\n",
    "#     image['image_id']: \n",
    "#             [attr for obj in image['attributes'] for attr in obj] for image in attributes}\n",
    "# print(attributes2[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for image in attributes:\n",
    "#     count+=1\n",
    "#     image_id = image['image_id']\n",
    "#     for obj in image['attributes']:\n",
    "#         try:\n",
    "#             obj_attributes = obj['attributes']\n",
    "#         except:\n",
    "#             print(count)\n",
    "#             print(obj)\n",
    "#             raise Exception(\"no attributes\")\n",
    "# print(attributes[5]['attributes'][0]['attributes'])\n",
    "# attribute_list = [[obj['attributes'] for obj in image['attributes']] for image in attributes]\n",
    "# print(attribute_list[5])\n",
    "# print(attributes[5]['attributes'][0]['names'])\n",
    "# name_list = [name for image in attributes for obj in image['attributes'] for name in obj['names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_descriptions2 = {r['id']: [region['phrase'] for region in  r['regions']] for r in region_descriptions}\n",
    "# object_descriptions2 = {r['image_id']: [object['name'] for object in  r['objects']] for r in region_descriptions}\n",
    "# attributes2 = {\n",
    "#     image['image_id']: \n",
    "#         [attr + \" \" + name \n",
    "#             for attr_desc_list in image['attributes'] for name in attr_desc_list['names'] for attr in attr_desc_list['attributes'] ] for image in attributes}\n",
    "# relationships2 = {r['image_id']: [relationship['subject']['name'] + \" \" + relationship['predicate'] + \" \" + relationship['object']['name'] for relationship in  r['relationships']] for r in relationships}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flowers hanging from the pole', 'Trash can beside the street', 'Bikes parked beside the pole', 'White bag being carried', 'Bus on the street', 'Bike parked beside the building', 'Vehicle parked on the sidewalk', 'windows in the building', 'Line on the street', 'Person wearing a pink shirt', 'the bikes are parked', 'the bus is colorful', 'the buildings in the city', 'the flowers hanging on the pole', 'the street light is tall', 'the wheels under the bus', 'the woman and child holding balloons', 'a colorful public bus', 'a person in a white shirt', 'a person in a white shirt', 'a person in a black shirt', 'a child wearing white', 'a small outdoor lamp', 'a white road bicycle', 'a white and red bicycle', 'a public garbage can', 'a person in a pink shirt', 'square window on building', 'square window on building', 'square window on building', 'square window on building', 'square window on building', 'square window on building', 'square window on building', 'bicycle parked against tree', 'bicycle parked against tree', 'garbage can by roadside', 'bus parked on road', 'person standing next to bus', 'person standing on sidewalk', 'person walking next to bus', 'person standing on sidewalk', 'building next to building']\n"
     ]
    }
   ],
   "source": [
    "print(region_descriptions2[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k')\n",
    "model.to(device)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-bigG-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_ids):\n",
    "    images = [get_image(image_id) for image_id in image_ids]\n",
    "    preprocessed_images = [preprocess(image) for image in images]\n",
    "    image_input = torch.tensor(np.stack(preprocessed_images)).to(device)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_featuress = model.encode_image(image_input) # .float()\n",
    "        image_featuress = [image_features/image_features.norm(dim=-1, keepdim=True) for image_features in image_featuress]\n",
    "    return image_featuress\n",
    "\n",
    "def encode_text(text):\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(tokenizer(text).to(device)) #.float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding images...\n",
      "done encoding images\n"
     ]
    }
   ],
   "source": [
    "print_feq = 1\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return [seq[pos:pos + size] for pos in range(0, len(seq), size)]\n",
    "print(\"encoding images...\")\n",
    "encoded_image_corpus = []\n",
    "for i,batch in enumerate(chunker(image_id_corpus, 100)):\n",
    "    batch_encoded = encode_images(batch)\n",
    "    encoded_image_corpus.extend(batch_encoded)\n",
    "    if i % print_feq == 0:\n",
    "        print(i,end='\\r')\n",
    "print(\"done encoding images\")\n",
    "\n",
    "ecoded_imgcorp_dict = {image_id: encoded_image.cpu() for image_id, encoded_image in zip(image_id_corpus, encoded_image_corpus)}\n",
    "del encoded_image_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting all descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 186008.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done getting all descriptions\n",
      "getting query descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 346949.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done getting query descriptions\n",
      "encoding descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done encoding descriptions\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# create dataset\n",
    "\n",
    "# load images, get embeddings\n",
    "\n",
    "# load descriptions\n",
    "\n",
    "# image_id_corpus = ...\n",
    "\n",
    "query_ids = np.random.choice(image_id_corpus, 10000)\n",
    "print_feq = 1\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return [seq[pos:pos + size] for pos in range(0, len(seq), size)]\n",
    "# print(\"encoding images...\")\n",
    "# encoded_image_corpus = []\n",
    "# for i,batch in enumerate(chunker(image_id_corpus, 10)):\n",
    "#     batch_encoded = encode_images(batch)\n",
    "#     encoded_image_corpus.extend(batch_encoded)\n",
    "#     if i % print_feq == 0:\n",
    "#         print(i,end='\\r')\n",
    "# print(\"done encoding images\")\n",
    "\n",
    "# ecoded_imgcorp_dict = {image_id: encoded_image for image_id, encoded_image in zip(image_id_corpus, encoded_image_corpus)}\n",
    "# del encoded_image_corpus\n",
    "\n",
    "\n",
    "def get_descriptions(query_id):\n",
    "    # objects = \n",
    "    # relations = ...\n",
    "    # thirdone = ...\n",
    "    # obj_descs = [...]\n",
    "    # rel_descs = [...]\n",
    "    # thirone_descs = [...]\n",
    "    descs = region_descriptions2[query_id] # obj_descs + rel_descs + thirone_descs\n",
    "    return descs\n",
    "\n",
    "print(\"getting all descriptions...\")\n",
    "all_descs = []\n",
    "for query_id in tqdm(image_id_corpus):\n",
    "    descs = get_descriptions(query_id)\n",
    "    all_descs.extend(descs)\n",
    "all_descs = list(set(all_descs))\n",
    "print(\"done getting all descriptions\")\n",
    "\n",
    "print(\"getting query descriptions...\")\n",
    "query_descs_true = []\n",
    "query_descs_false = []\n",
    "for query_id in tqdm(query_ids):\n",
    "    descs = get_descriptions(query_id)\n",
    "    true_idx = randint(0, len(descs)-1)\n",
    "    description = descs[true_idx]\n",
    "    query_descs_true.append(description)\n",
    "    false_idx = randint(0, len(all_descs)-1)\n",
    "    query_descs_false.append(all_descs[false_idx])\n",
    "print(\"done getting query descriptions\")\n",
    "\n",
    "print(\"encoding descriptions...\")\n",
    "encoded_descs_true = []\n",
    "encoded_descs_false = []\n",
    "for descriptions in chunker(query_descs_true, 100):\n",
    "    batch_encoded = encode_text(descriptions)\n",
    "    encoded_descs_true.extend(batch_encoded)\n",
    "for descriptions in chunker(query_descs_false, 100):\n",
    "    batch_encoded = encode_text(descriptions)\n",
    "    encoded_descs_false.extend(batch_encoded)\n",
    "print(\"done encoding descriptions\")\n",
    "\n",
    "# filtering out images that have the object is pointless at this stage, because the descriptions are not exactly the same for similar objects\n",
    "image_ids = np.random.choice(image_id_corpus, 10000*50).reshape(10000, 50)\n",
    "encoded_descs_true = [desc.cpu() for desc in encoded_descs_true]\n",
    "encoded_descs_false = [desc.cpu() for desc in encoded_descs_false]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:22, 439.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_x = []\n",
    "dataset_y = []\n",
    "for query_id,desc_true,desc_false,comp_ids  in tqdm(zip(query_ids, encoded_descs_true, encoded_descs_false, image_ids)):\n",
    "    query_enc = ecoded_imgcorp_dict[query_id]\n",
    "    comp_encs = [ecoded_imgcorp_dict[comp_id] for comp_id in comp_ids]\n",
    "    query_similarity_true = cos_sim(query_enc, desc_true, dim=-1)\n",
    "    query_similarity_false = cos_sim(query_enc, desc_false, dim=-1)\n",
    "    comp_similarities_true = [cos_sim(comp_enc, desc_true, dim=-1) for comp_enc in comp_encs]\n",
    "    comp_similarities_false = [cos_sim(comp_enc, desc_false, dim=-1) for comp_enc in comp_encs]\n",
    "    dataset_x.append( (comp_similarities_true,query_similarity_true) )\n",
    "    dataset_y.append(1)\n",
    "    dataset_x.append( (comp_similarities_false,query_similarity_false) )\n",
    "    dataset_y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile(comp_similarities, query_similarity):\n",
    "    n_higher = sum([similarity >= query_similarity for similarity in comp_similarities])\n",
    "    return n_higher / len(image_ids)\n",
    "\n",
    "def get_std_distance(comp_similarities, query_similarity):\n",
    "    mean = np.mean(comp_similarities)\n",
    "    std = np.std(comp_similarities)\n",
    "    return (query_similarity - mean) / std\n",
    "\n",
    "X_percentiles = [get_percentile(comp_similarities, query_similarity) for comp_similarities, query_similarity in dataset_x]\n",
    "X_std_distances = [get_std_distance(comp_similarities, query_similarity) for comp_similarities, query_similarity in dataset_x]\n",
    "Y = dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "toy_data_x = np.array([0.0,1.0,2.0,3.0]).reshape(-1,1)\n",
    "toy_data_y = [0,0,1,1]\n",
    "svm = LinearSVC(loss = 'hinge', C = 10000)\n",
    "svm.fit(toy_data_x, toy_data_y)\n",
    "svm.predict(np.array([0.5,1.49,1.51, 2.5]).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81375\n",
      "0.83025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(torch.stack(X_percentiles).reshape(-1,1), Y, test_size=0.2, random_state=42)\n",
    "svm_percentile = LinearSVC(loss = 'hinge', C = 10000, max_iter=1000000)\n",
    "svm_percentile.fit(X_train, Y_train)\n",
    "accuracy_percentile = svm_percentile.score(X_test, Y_test)\n",
    "print(accuracy_percentile)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(torch.stack(X_std_distances).reshape(-1,1), Y, test_size=0.2, random_state=42)\n",
    "svm_std_distance = LinearSVC(loss = 'hinge', C = 10, max_iter=1000000)\n",
    "svm_std_distance.fit(X_train, Y_train)\n",
    "accuracy_std_distance = svm_std_distance.score(X_test, Y_test)\n",
    "print(accuracy_std_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.09524066]\n",
      "[[-952.40658825]]\n",
      "[-1.12951556]\n",
      "[[0.92780488]]\n"
     ]
    }
   ],
   "source": [
    "print(svm_percentile.intercept_)\n",
    "print(svm_percentile.coef_)\n",
    "\n",
    "print(svm_std_distance.intercept_)\n",
    "print(svm_std_distance.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecbca5960225455e7ce72c399dae152ce4c3acba6653762963b551bcc9c7a957"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
